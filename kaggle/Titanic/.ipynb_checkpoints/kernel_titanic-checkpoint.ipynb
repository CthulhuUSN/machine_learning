{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy\n",
    "import warnings\n",
    "import math\n",
    "from collections import namedtuple\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import TitanicUtils as tu\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for the data processing\n",
    "def substrings_in_string(big_string, substrings):\n",
    "    for substring in substrings:\n",
    "        if str.find(big_string, substring) != -1:\n",
    "            return substring\n",
    "    #print(big_string)\n",
    "    return np.nan\n",
    "\n",
    "def replace_titles(x):\n",
    "    title=x['Title']\n",
    "    if title in ['Don', 'Major', 'Capt', 'Jonkheer', 'Rev', 'Col', 'Mr', 'Master']:\n",
    "        return -1\n",
    "    elif title in ['Countess', 'Mme', 'Mrs']:\n",
    "        return 0\n",
    "    elif title in ['Mlle', 'Ms', 'Miss']:\n",
    "        return 1\n",
    "    elif title =='Dr':\n",
    "        if x['Sex']==1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    else:\n",
    "        return title \n",
    "\n",
    "#The data processing for the training and testing data sets\n",
    "def data_train():\n",
    "    train_data = pd.read_csv(\"train.csv\")\n",
    "    \n",
    "    #Turning cabin number into Deck\n",
    "    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n",
    "    train_data['Cabin'] = train_data['Cabin'].astype(str)\n",
    "    train_data['Deck'] = train_data['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "    \n",
    "    train_data.loc[(train_data.Deck == 'A'),'Deck'] = -1\n",
    "    train_data.loc[(train_data.Deck == 'B'),'Deck'] = -.75\n",
    "    train_data.loc[(train_data.Deck == 'C'),'Deck'] = -.5\n",
    "    train_data.loc[(train_data.Deck == 'D'),'Deck'] = -.25\n",
    "    train_data.loc[(train_data.Deck == 'E'),'Deck'] = .25\n",
    "    train_data.loc[(train_data.Deck == 'F'),'Deck'] = .5\n",
    "    train_data.loc[(train_data.Deck == 'T'),'Deck'] = .75\n",
    "    train_data.loc[(train_data.Deck == 'G'),'Deck'] = 1\n",
    "    train_data[\"Deck\"] = train_data[\"Deck\"].fillna(0)\n",
    "    \n",
    "    #Creating new family_size column\n",
    "    train_data['Family_Size'] = train_data['SibSp'] + train_data['Parch']\n",
    "    \n",
    "    train_data['Age*Class'] = train_data['Age'] * train_data['Pclass']\n",
    "    \n",
    "    train_data['Fare_Per_Person'] = train_data['Fare']  / (train_data['Family_Size']+1)\n",
    "\n",
    "    train_data.loc[(train_data.Sex == 'female'),'Sex'] = -1\n",
    "    train_data.loc[(train_data.Sex == 'male'),'Sex'] = 1\n",
    "\n",
    "    train_data.loc[(train_data.Embarked == 'C'),'Embarked'] = -1\n",
    "    train_data.loc[(train_data.Embarked == 'Q'),'Embarked'] = 1\n",
    "    train_data.loc[(train_data.Embarked == 'S'),'Embarked'] = 2\n",
    "    train_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(0)\n",
    "\n",
    "    train_data.loc[(train_data.Pclass == 1),'Pclass'] = 1\n",
    "    train_data.loc[(train_data.Pclass == 2),'Pclass'] = 0\n",
    "    train_data.loc[(train_data.Pclass == 3),'Pclass'] = -1\n",
    "    \n",
    "    train_data['Sex*Class'] = train_data['Sex'] * train_data['Pclass']\n",
    "    \n",
    "    train_data[\"Age\"] = train_data[\"Age\"].fillna(0)\n",
    "    \n",
    "    train_data[\"Age*Class\"] = train_data[\"Age*Class\"].fillna(0)\n",
    "    \n",
    "    train_data['Embarked*Class'] = train_data['Embarked'] * train_data['Pclass']\n",
    "    \n",
    "    train_data['Age*Deck'] = train_data['Age'] * train_data['Deck']\n",
    "    \n",
    "    #train_data[\"Embarked*Class\"] = train_data[\"Embarked*Class\"].fillna(0)\n",
    "\n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "                        'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "                        'Don', 'Jonkheer']\n",
    "\n",
    "    train_data['Title'] = train_data['Name'].map(lambda x: substrings_in_string(x, title_list))   \n",
    "    train_data['Title'] = train_data.apply(replace_titles, axis=1)\n",
    "    \n",
    "    train_data['Title*Class'] = train_data['Title'] * train_data['Pclass']\n",
    "\n",
    "    tmp = train_data[['Pclass', 'Title', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Deck', 'Title*Class',\n",
    "                      'Family_Size', 'Age*Class', 'Fare_Per_Person', 'Embarked*Class', 'Sex*Class', 'Age*Deck']].copy()\n",
    "    \n",
    "    X = pd.DataFrame(tmp).to_numpy()\n",
    "    X = X.T\n",
    "    Y = np.array([train_data['Survived'].values])\n",
    "\n",
    "    X = np.float32(X)\n",
    "    Y = np.float32(Y)\n",
    "    m = X.shape[1]  \n",
    "    #Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuff_X = X[:, permutation]\n",
    "    shuff_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    devM = round(m * .05)\n",
    "    devM2 = m - devM\n",
    "    \n",
    "    train_X = shuff_X[:, 0:(devM2)]\n",
    "    train_Y = shuff_Y[:, 0:(devM2)]\n",
    "    dev_X = shuff_X[:, -(devM + 1):-1]\n",
    "    dev_Y = shuff_Y[:, -(devM + 1):-1]\n",
    "    \n",
    "    m_train = train_X.shape[1]\n",
    "    m_test = dev_X.shape[1]\n",
    "    \n",
    "    print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "    print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "    print (\"train_set_x shape: \" + str(train_X.shape))\n",
    "    print (\"train_set_y shape: \" + str(train_Y.shape))\n",
    "    print (\"test_set_x shape: \" + str(dev_X.shape))\n",
    "    print (\"test_set_y shape: \" + str(dev_Y.shape))\n",
    "    \n",
    "    return train_X, train_Y, dev_X, dev_Y\n",
    "\n",
    "#The data processing for the submission data sets\n",
    "def data_test():\n",
    "    test_data = pd.read_csv(\"test.csv\")\n",
    "    \n",
    "    #Turning cabin number into Deck\n",
    "    cabin_list = ['A', 'B', 'C', 'D', 'E', 'F', 'T', 'G', 'Unknown']\n",
    "    test_data['Cabin'] = test_data['Cabin'].astype(str)\n",
    "    test_data['Deck'] = test_data['Cabin'].map(lambda x: substrings_in_string(x, cabin_list))\n",
    "    \n",
    "    test_data.loc[(test_data.Deck == 'A'),'Deck'] = -1\n",
    "    test_data.loc[(test_data.Deck == 'B'),'Deck'] = -.75\n",
    "    test_data.loc[(test_data.Deck == 'C'),'Deck'] = -.5\n",
    "    test_data.loc[(test_data.Deck == 'D'),'Deck'] = -.25\n",
    "    test_data.loc[(test_data.Deck == 'E'),'Deck'] = .25\n",
    "    test_data.loc[(test_data.Deck == 'F'),'Deck'] = .5\n",
    "    test_data.loc[(test_data.Deck == 'T'),'Deck'] = .75\n",
    "    test_data.loc[(test_data.Deck == 'G'),'Deck'] = 1\n",
    "    test_data[\"Deck\"] = test_data[\"Deck\"].fillna(0)\n",
    "    \n",
    "    #Creating new family_size column\n",
    "    test_data['Family_Size'] = test_data['SibSp'] + test_data['Parch']\n",
    "    \n",
    "    test_data['Age*Class'] = test_data['Age'] * test_data['Pclass']\n",
    "    \n",
    "    test_data['Fare_Per_Person'] = test_data['Fare']  / (test_data['Family_Size']+1)\n",
    "\n",
    "    test_data.loc[(test_data.Sex == 'female'),'Sex'] = -1\n",
    "    test_data.loc[(test_data.Sex == 'male'),'Sex'] = 1\n",
    "\n",
    "    test_data.loc[(test_data.Embarked == 'C'),'Embarked'] = -1\n",
    "    test_data.loc[(test_data.Embarked == 'Q'),'Embarked'] = 1\n",
    "    test_data.loc[(test_data.Embarked == 'S'),'Embarked'] = 2\n",
    "    test_data[\"Embarked\"] = test_data[\"Embarked\"].fillna(0)\n",
    "\n",
    "    test_data.loc[(test_data.Pclass == 1),'Pclass'] = 1\n",
    "    test_data.loc[(test_data.Pclass == 2),'Pclass'] = 0\n",
    "    test_data.loc[(test_data.Pclass == 3),'Pclass'] = -1\n",
    "    \n",
    "    test_data['Sex*Class'] = test_data['Sex'] * test_data['Pclass']\n",
    "    \n",
    "    test_data[\"Age\"] = test_data[\"Age\"].fillna(0)\n",
    "    \n",
    "    test_data[\"Age*Class\"] = test_data[\"Age*Class\"].fillna(0)\n",
    "    \n",
    "    test_data['Embarked*Class'] = test_data['Embarked'] * test_data['Pclass']\n",
    "    \n",
    "    test_data['Age*Deck'] = test_data['Age'] * test_data['Deck']\n",
    "    \n",
    "    #test_data[\"Embarked*Class\"] = test_data[\"Embarked*Class\"].fillna(0)\n",
    "\n",
    "    title_list=['Mrs', 'Mr', 'Master', 'Miss', 'Major', 'Rev',\n",
    "                        'Dr', 'Ms', 'Mlle','Col', 'Capt', 'Mme', 'Countess',\n",
    "                        'Don', 'Jonkheer']\n",
    "\n",
    "    test_data['Title'] = test_data['Name'].map(lambda x: substrings_in_string(x, title_list))   \n",
    "    test_data['Title'] = test_data.apply(replace_titles, axis=1)\n",
    "    \n",
    "    test_data['Title*Class'] = test_data['Title'] * test_data['Pclass']\n",
    "\n",
    "    tmp = test_data[['Pclass', 'Title', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Deck', 'Title*Class',\n",
    "                      'Family_Size', 'Age*Class', 'Fare_Per_Person', 'Embarked*Class', 'Sex*Class', 'Age*Deck']].copy()\n",
    "    \n",
    "    X = pd.DataFrame(tmp).to_numpy()\n",
    "    X = X.T\n",
    "\n",
    "    X = np.float32(X)\n",
    "    \n",
    "    test = X[:, :]\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, tmpcache, lambd = 0):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat)\n",
    "    cache -- cache output from forward_propagation()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    cache = {}\n",
    "    (a0, z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3, z4, a4, W4, b4, z5, a5, W5, b5, z6, a6, W6, b6) = tmpcache\n",
    "\n",
    "    for l in range(6):\n",
    "        zkey = 'z' + str(l+1)\n",
    "        akey = 'a' + str(l+1)\n",
    "        Wkey = 'W' + str(l+1)\n",
    "        bkey = 'b' + str(l+1)\n",
    "        cache[Wkey] = tmpcache[Wkey]\n",
    "        cache[bkey] = tmpcache[bkey]\n",
    "        cache[zkey] = tmpcache[zkey]\n",
    "        cache[akey] = tmpcache[akey]\n",
    "\n",
    "        \n",
    "    #dW3 = 1./m * np.dot(dZ3, A2.T) + ((lambd / m) * W3)\n",
    "    \n",
    "    if lambd > 0:\n",
    "        dz6 = 1./m * (cache['a6'] - Y)\n",
    "        dW6 = np.dot(dz6, cache['a5'].T) + ((lambd / m) * cache['W6'])\n",
    "        db6 = np.sum(dz6, axis=1, keepdims = True)  \n",
    "        dz5 = np.dot(cache['W6'].T, dz6) * (1 - np.power(cache['a5'], 2))\n",
    "        dW5 = np.dot(dz5, cache['a4'].T) + ((lambd / m) * cache['W5'])\n",
    "        db5 = np.sum(dz5, axis=1, keepdims = True)\n",
    "        dz4 = np.dot(cache['W5'].T, dz5) * (1 - np.power(cache['a4'], 2))\n",
    "        dW4 = np.dot(dz4, cache['a3'].T) + ((lambd / m) * cache['W4'])\n",
    "        db4 = np.sum(dz4, axis=1, keepdims = True)\n",
    "        dz3 = np.dot(cache['W4'].T, dz4) * (1 - np.power(cache['a3'], 2))\n",
    "        dW3 = np.dot(dz3, cache['a2'].T) + ((lambd / m) * cache['W3'])\n",
    "        db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "        dz2 = np.dot(cache['W3'].T, dz3) * (1 - np.power(cache['a2'], 2))\n",
    "        dW2 = np.dot(dz2, cache['a1'].T) + ((lambd / m) * cache['W2'])\n",
    "        db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "        dz1 = np.dot(cache['W2'].T, dz2) * (1 - np.power(cache['a1'], 2))\n",
    "        dW1 = np.dot(dz1, X.T) + ((lambd / m) * cache['W1'])\n",
    "        db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    else:\n",
    "        dz6 = 1./m * (cache['a6'] - Y)\n",
    "        dW6 = np.dot(dz6, cache['a5'].T)\n",
    "        db6 = np.sum(dz6, axis=1, keepdims = True)  \n",
    "        dz5 = np.dot(cache['W6'].T, dz6) * (1 - np.power(cache['a5'], 2))\n",
    "        dW5 = np.dot(dz5, cache['a4'].T)\n",
    "        db5 = np.sum(dz5, axis=1, keepdims = True)\n",
    "        dz4 = np.dot(cache['W5'].T, dz5) * (1 - np.power(cache['a4'], 2))\n",
    "        dW4 = np.dot(dz4, cache['a3'].T)\n",
    "        db4 = np.sum(dz4, axis=1, keepdims = True)\n",
    "        dz3 = np.dot(cache['W4'].T, dz4) * (1 - np.power(cache['a3'], 2))\n",
    "        dW3 = np.dot(dz3, cache['a2'].T)\n",
    "        db3 = np.sum(dz3, axis=1, keepdims = True)\n",
    "        dz2 = np.dot(cache['W3'].T, dz3) * (1 - np.power(cache['a2'], 2))\n",
    "        dW2 = np.dot(dz2, cache['a1'].T)\n",
    "        db2 = np.sum(dz2, axis=1, keepdims = True)\n",
    "        dz1 = np.dot(cache['W2'].T, dz2) * (1 - np.power(cache['a1'], 2))\n",
    "        dW1 = np.dot(dz1, X.T)\n",
    "        db1 = np.sum(dz1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dz6\": dz6, \"dW6\": dW6, \"db6\": db6,\n",
    "                 \"dz5\": dz5, \"dW5\": dW5, \"db5\": db5,\n",
    "                 \"dz4\": dz4, \"dW4\": dW4, \"db4\": db4,\n",
    "                 \"dz3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, beta = 0.9, lambd = 0,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = tu.initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = tu.initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = tu.initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "\n",
    "        # Forward propagation and back prop\n",
    "        if lambd > 0:\n",
    "            cost, cache = tu.forward_propagation(X, Y, parameters, lambd)\n",
    "            grads = backward_propagation(X, Y, cache, lambd)\n",
    "        else:\n",
    "            cost, cache = tu.forward_propagation(X, Y, parameters)\n",
    "            grads = backward_propagation(X, Y, cache)\n",
    "\n",
    "        # Update parameters\n",
    "        if optimizer == \"gd\":\n",
    "            parameters = tu.update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "        elif optimizer == \"momentum\":\n",
    "            parameters, v = tu.update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "        elif optimizer == \"adam\":\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s = tu.update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "\n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26281864019752443 0.09199904522700109 -1.3076660128732143 0.21287768171914198\n"
     ]
    }
   ],
   "source": [
    "X_assess, Y, parameters = tu.forward_propagation_test_case()\n",
    "_, cache = tu.forward_propagation(X_assess, Y, parameters)\n",
    "\n",
    "# Note: we use the mean here just to make sure that your output matches ours. \n",
    "print(np.mean(cache['z1']) ,np.mean(cache['a1']),np.mean(cache['z2']),np.mean(cache['a2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table style=\"width:50%\">\n",
    "  <tr>\n",
    "    <td> 0.262818640198 0.091999045227 -1.30766601287 0.212877681719 </td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 1.7864859451590758\n"
     ]
    }
   ],
   "source": [
    "A3, Y_assess, parameters = tu.compute_cost_with_regularization_test_case()\n",
    "\n",
    "print(\"cost = \" + str(tu.compute_cost_with_regularization(A3, Y_assess, parameters, lambd = 0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table> \n",
    "    <tr>\n",
    "    <td>\n",
    "    **cost**\n",
    "    </td>\n",
    "        <td>\n",
    "    1.78648594516\n",
    "    </td>\n",
    "    </tr>\n",
    "\n",
    "</table> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mYour backward propagation works perfectly fine! difference = 9.86774593024366e-09\u001b[0m\n",
      "\u001b[92mYour backward propagation works perfectly fine! difference = 1.6953480311484942e-07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = tu.gradient_check_test_case()\n",
    "cost, cache = tu.forward_propagation(X, Y, parameters)\n",
    "gradients = backward_propagation(X, Y, cache)\n",
    "difference = tu.gradient_check(parameters, gradients, X, Y)\n",
    "lambd = 0.7\n",
    "gradients = backward_propagation(X, Y, cache, lambd = lambd)\n",
    "difference = tu.gradient_check(parameters, gradients, X, Y, lambd = lambd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 846\n",
      "Number of testing examples: m_test = 45\n",
      "train_set_x shape: (16, 846)\n",
      "train_set_y shape: (1, 846)\n",
      "test_set_x shape: (16, 45)\n",
      "test_set_y shape: (1, 45)\n",
      "Cost after epoch 0: 0.719571\n",
      "Cost after epoch 1000: 0.375725\n",
      "Cost after epoch 2000: 0.330159\n",
      "Cost after epoch 3000: 0.325598\n",
      "Cost after epoch 4000: 0.324747\n",
      "Cost after epoch 5000: 0.324423\n",
      "Cost after epoch 6000: 0.324174\n",
      "Cost after epoch 7000: 0.323972\n",
      "Cost after epoch 8000: 0.320023\n",
      "Cost after epoch 9000: 0.319706\n",
      "Cost after epoch 10000: 0.331446\n",
      "Cost after epoch 11000: 0.324017\n",
      "Cost after epoch 12000: 0.319190\n",
      "Cost after epoch 13000: 0.318951\n",
      "Cost after epoch 14000: 0.318812\n",
      "Cost after epoch 15000: 0.321125\n",
      "Cost after epoch 16000: 0.313899\n",
      "Cost after epoch 17000: 0.313790\n",
      "Cost after epoch 18000: 0.316584\n",
      "Cost after epoch 19000: 0.313699\n",
      "Cost after epoch 20000: 0.316466\n",
      "Cost after epoch 21000: 0.316440\n",
      "Cost after epoch 22000: 0.316373\n",
      "Cost after epoch 23000: 0.313535\n",
      "Cost after epoch 24000: 0.316317\n",
      "Cost after epoch 25000: 0.313446\n",
      "Cost after epoch 26000: 0.313406\n",
      "Cost after epoch 27000: 0.318995\n",
      "Cost after epoch 28000: 0.324676\n",
      "Cost after epoch 29000: 0.316109\n",
      "Cost after epoch 30000: 0.313311\n",
      "Cost after epoch 31000: 0.313271\n",
      "Cost after epoch 32000: 0.331384\n",
      "Cost after epoch 33000: 0.313205\n",
      "Cost after epoch 34000: 0.321211\n",
      "Cost after epoch 35000: 0.313187\n",
      "Cost after epoch 36000: 0.313126\n",
      "Cost after epoch 37000: 0.313123\n",
      "Cost after epoch 38000: 0.315518\n",
      "Cost after epoch 39000: 0.313034\n",
      "Cost after epoch 40000: 0.310549\n",
      "Cost after epoch 41000: 0.310509\n",
      "Cost after epoch 42000: 0.310463\n",
      "Cost after epoch 43000: 0.310527\n",
      "Cost after epoch 44000: 0.310461\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9066193853427896\n",
      "Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, dev_X, dev_Y = data_train()\n",
    "\n",
    "X = train_X\n",
    "Y = train_Y\n",
    "layers_dims = [train_X.shape[0], 32, 16, 8, 4, 2, 1]\n",
    "optimizer = \"adam\"\n",
    "learning_rate = 0.0007\n",
    "beta = 0.9\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "num_epochs = 45000\n",
    "print_cost = True\n",
    "lambd = .07\n",
    "\n",
    "parameters = model(X, Y, layers_dims, optimizer = optimizer, learning_rate = learning_rate, num_epochs = num_epochs, \n",
    "                   lambd = lambd, beta = beta, beta1 = beta1, beta2 = beta2, epsilon = epsilon, print_cost = print_cost)\n",
    "\n",
    "# Predict\n",
    "predictions = tu.predict(train_X, train_Y, parameters)\n",
    "predictions1 = tu.predict(dev_X, dev_Y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your submission was successfully saved!\n"
     ]
    }
   ],
   "source": [
    "test_X = data_test()\n",
    "results = tu.predict_t(test_X, parameters)\n",
    "results = results.T\n",
    "predictions = results.tolist()\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n",
    "output['Survived'] = output['Survived'].str.get(0)\n",
    "output.to_csv('my_submission.csv', index=False)\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
